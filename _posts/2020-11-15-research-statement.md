---
layout: post
title: "Research Statement"

---

Recent developments in the field of artificial intelligence (AI) have made me extremely excited about where the field is heading. Over the past few years, GPT-3 and related deep transformer models have enabled state-of-the-art results across natural language processing and computer vision. These transformer models perform impressively on many tasks, such as dialogue [1], object detection [2], code completion [3], and theorem proving [4]. However, because the models are not grounded in any formal semantic representation, they often generate nonsensical output [5], showing that there are likely limitations to the types of reasoning that will emerge, especially as we reach bottlenecks on quantity and types of training data. Therefore, a natural next step is to augment these models with some form of commonsense reasoning and knowledge. How exactly to do this is unclear, but there are many approaches that have shown promising results, including multimodal perception [6, 7, 8], grounded language understanding [8, 9], automated knowledge base construction [10, 11], and interactive task learning [12].

One path that I am especially intrigued by is the use of virtual environments as a means of training and testing agents, as different types of virtual environments can be used to test different capabilities of reinforcement learning agents. Recently, there has been work exploring multi-agent environments [13, 14], open-world sandbox environments [15], text-based environments [16, 17], and environments mimicking the real-world [18, 19]. However, most of the work in this area has relied on purely statistical (deep learning) approaches, and as alluded to above, it is unlikely that deep learning alone will get us where we want to go. And even if hybrid statistical-symbolic approaches aren’t necessary to achieve more general AI, there is good reason to believe that such approaches will prove more successful in the long run [20]. New technologies like graph convolutional networks [21], knowledge graph embeddings [22], and cognitive architectures [23] have started to bridge the gap toward hybrid systems. To me, these hybrid systems represent the most promising path toward general AI, and during my graduate work, I hope to expand on this work in order to build systems capable of operating in increasingly complex environments.


### References

[1] Ham, Donghoon, et al. "End-to-End Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2." ACL, 2020.

[2] Carion, Nicolas, et al. "End-to-End Object Detection with Transformers." arXiv preprint arXiv:2005.12872 (2020).

[3] Kim, Seohyun, et al. "Code Prediction by Feeding Trees to Transformers." arXiv preprint arXiv:2003.13848 (2020).

[4] Polu, Stanislas, and Ilya Sutskever. "Generative language modeling for automated theorem proving." arXiv preprint arXiv:2009.03393 (2020).

[5] Marcus, Gary. “GPT-3, Bloviator: OpenAI's Language Generator Has No Idea What It's Talking About.” MIT Technology Review, MIT Technology Review, 9 Sept. 2020, www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/.

[6] Wang, Xiaolong, Yufei Ye, and Abhinav Gupta. "Zero-shot recognition via semantic embeddings and knowledge graphs." Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.

[7] Liang, Junwei, et al. "Focal visual-text attention for memex question answering." IEEE transactions on pattern analysis and machine intelligence 41.8 (2019): 1893-1908.

[8] Paul, Rohan, et al. "Temporal grounding graphs for language understanding with accrued visual-linguistic context." arXiv preprint arXiv:1811.06966 (2018).

[9] Lindes, Peter, et al. "Grounding language for interactive task learning." Proceedings of the First Workshop on Language Grounding for Robotics. 2017.

[10] Bosselut, Antoine, et al. "COMET: Commonsense transformers for automatic knowledge graph construction." arXiv preprint arXiv:1906.05317 (2019).

[11] Adhikari, Ashutosh, et al. "Learning dynamic knowledge graphs to generalize on text-based games." arXiv preprint arXiv:2002.09127 (2020).

[12] Kirk, James R., and John E. Laird. "Learning Hierarchical Symbolic Representations to Support Interactive Task Learning and Knowledge Transfer." IJCAI. 2019.

[13] Bansal, Trapit, et al. "Emergent complexity via multi-agent competition." arXiv preprint arXiv:1710.03748 (2017).

[14] Suarez, Joseph, et al. "Neural MMO: A massively multiagent game environment for training and evaluating intelligent agents." arXiv preprint arXiv:1903.00784 (2019).

[15] Guss, William H., et al. "The minerl competition on sample efficient reinforcement learning using human priors." arXiv preprint arXiv:1904.10079 (2019).

[16] Côté, Marc-Alexandre, et al. "Textworld: A learning environment for text-based games." Workshop on Computer Games. Springer, Cham, 2018.

[17] Narasimhan, Karthik, Tejas Kulkarni, and Regina Barzilay. "Language understanding for text-based games using deep reinforcement learning." arXiv preprint arXiv:1506.08941 (2015).

[18] Puig, Xavier, et al. "Virtualhome: Simulating household activities via programs." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.

[19] Kaspar, Manuel, Juan David Munoz Osorio, and Jürgen Bock. "Sim2Real Transfer for Reinforcement Learning without Dynamics Randomization." arXiv preprint arXiv:2002.11635 (2020).

[20] Maruyama, Yoshihiro. "The Conditions of Artificial General Intelligence: Logic, Autonomy, Resilience, Integrity, Morality, Emotion, Embodiment, and Embeddedness." International Conference on Artificial General Intelligence. Springer, Cham, 2020.

[21] Kipf, Thomas N., and Max Welling. "Semi-supervised classification with graph convolutional networks." arXiv preprint arXiv:1609.02907 (2016).

[22] Wang, Zhen, et al. "Knowledge graph embedding by translating on hyperplanes." Aaai. Vol. 14. No. 2014. 2014.

[23] Mao, Jiayuan, et al. "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision." arXiv preprint arXiv:1904.12584 (2019).
